{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidheesh-p/AI-Learning/blob/master/the_need_for_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS3n2y8wPw5g"
      },
      "source": [
        "## The Need for RAG\n",
        "Before we start coding, lets go over a few questions and get it clarified.\n",
        "\n",
        "### 1. What is RAG?\n",
        "Imagine you're writing an article about climate change, but instead of relying only on what you remember, you search online for recent studies and data to support your writing. RAG works the same way—it combines a powerful AI language model with a search system that retrieves relevant information from an external knowledge source, like a database or documents. This helps generate more accurate and informative responses.\n",
        "\n",
        "### 2. Why is RAG important?\n",
        "RAG is crucial because AI models, like ChatGPT, can sometimes \"hallucinate\" or provide outdated or incorrect information. By retrieving facts from trusted sources before generating responses, RAG ensures the answers are more reliable, up-to-date, and contextually relevant.\n",
        "\n",
        "### 3. What’s the difference between RAG and a standard AI chatbot?\n",
        "A standard chatbot relies only on pre-trained knowledge, which may be limited or outdated. RAG-enhanced chatbots, however, actively retrieve fresh, relevant information from external sources, ensuring better accuracy and up-to-date insights.\n",
        "\n",
        "### 4. What are the key components of RAG?\n",
        "RAG consists of two main parts:\n",
        "\n",
        "- Retriever: Finds the most relevant documents or data from a knowledge base (e.g., a search engine or database).\n",
        "- Generator: Uses the retrieved information to produce a coherent and accurate response.\n",
        "\n",
        "### 5. Usecases in real-life\n",
        "- Customer Support: AI chatbots retrieve knowledge base articles to provide better responses to customer queries.\n",
        "- Healthcare: Doctors can get AI-assisted summaries of patient records and the latest medical research.\n",
        "- Legal Services: Lawyers can search through legal documents and case studies to build stronger cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-9ZmExzDRva"
      },
      "source": [
        "### Understanding the Limitation of the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRit5A9KCx20"
      },
      "outputs": [],
      "source": [
        "# Importing the OpenAI library to interact with OpenAI's API services.\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKBjz45zDj6r",
        "outputId": "a2f92ca8-2b04-478e-f107-ca022644e90f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os  # Importing the os module to interact with environment variables\n",
        "import getpass  # Importing getpass to securely input sensitive information\n",
        "\n",
        "# Prompting the user to securely enter their OpenAI API key without displaying it on the screen\n",
        "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7YY7aEjDj9I"
      },
      "outputs": [],
      "source": [
        "# Creating an instance of the OpenAI client using the provided API key.\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtvWPnanDkAj"
      },
      "outputs": [],
      "source": [
        "# Defining the prompt to query the LLM\n",
        "prompt = ''' What was uber's revenue in 2022? '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBFCA_A_DWBp"
      },
      "outputs": [],
      "source": [
        "# Sending a request to the OpenAI API to generate a chat response\n",
        "openai_response = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',  # Specifying the model to use;\n",
        "    # Note: An older model chosen for testing purposes because the cutoff is 2021 whereas prompt is querying details about 2022\n",
        "    messages=[{'role': 'user', 'content': prompt}]  # Creating a structured message for the AI model\n",
        "    # use: what was uber's revnue...\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WONjF1M1Xs-k"
      },
      "source": [
        "In the above code, while creating a structured message for the AI model,  `role `defines the speaker (user input) and `content` contains the actual query stored in the `prompt` variable.\n",
        "\n",
        "We are structuring the input this way because OpenAI's chat models require a specific format to understand and process conversations effectively. Assigning roles like 'user' helps the AI distinguish between different participants in the conversation, ensuring it provides relevant and context-aware responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u-Y1hWlDij2",
        "outputId": "567d106a-6a5d-4578-d88c-1f39a17fe6be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of now, it is not possible to provide the exact revenue for Uber in 2022 as it is still early in the year. Typically, companies release their revenue figures at the end of the fiscal year or quarterly. You may want to check Uber's financial reports or official announcements for the most recent revenue information.\n"
          ]
        }
      ],
      "source": [
        "# Accessing the generated response from the AI model.\n",
        "print(openai_response.choices[0].message.content)\n",
        "# Note:'choices' contains multiple response options, we take the first one ([0]),\n",
        "# 'message' holds the response details, and 'content' extracts the actual text generated by the AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJLYV5B_YWGD"
      },
      "source": [
        "### Interpretation:\n",
        "Why is the LLM not able to answer the query?\n",
        "\n",
        "`gpt-3.5-turbo` does not have access to data after 2021 because of its cutoff.\n",
        "\n",
        "#### Do it yourself:\n",
        "Try changing the model to `gpt-4o-mini` and observe how the output changes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl4cl9ylD7AF"
      },
      "source": [
        "As you can see above, the LLM we used(gpt 3.5) doesn't have access to the latest data. Now as LLMs get updated, the training cut-off date may or may not have access to more information. However, it's always a good idea to understand how to improve the context of our prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXHhy-hcEMyy"
      },
      "source": [
        "### Making the LLM context-aware\n",
        "\n",
        "Next Step: Let's check Uber's [financial report ](https://s23.q4cdn.com/407969754/files/doc_events/2024/May/06/2023-annual-report.pdf)\n",
        "\n",
        "On Page 54, of the above document it states:\n",
        "\n",
        "\"Revenue was 37.3 billion, up 17% year-over-year. Mobility revenue increased 5.8 billion primarily attributable to an increase in\n",
        "Mobility Gross Bookings of......\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSAk_2EGD1CC"
      },
      "outputs": [],
      "source": [
        "## Let's create the above context for the prompt\n",
        "# Defining a context string with revenue details retrieved from an external source.\n",
        "retrieved_context = '''Revenue was $37.3 billion, up 17% year-over-year. Mobility revenue increased $5.8 billion primarily attributable to an increase in\n",
        "               Mobility Gross Bookings of 31% year-over-year.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irw2unBKFb0M"
      },
      "outputs": [],
      "source": [
        "## Let's modify our prompt now\n",
        "# Creating a prompt by embedding the retrieved context into a question for the AI model.\n",
        "\n",
        "prompt = f\"What was Uber's revenue in 2022? Check in {retrieved_context}\"\n",
        "\n",
        "# Note: The AI is being asked to analyze the given context and provide Uber's revenue for 2022"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cmJSGdKfQt7",
        "outputId": "e8bb5d4d-af73-4eb9-a8e4-dd0365cc950d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What was Uber's revenue in 2022? Check in Revenue was $37.3 billion, up 17% year-over-year. Mobility revenue increased $5.8 billion primarily attributable to an increase in\n",
            "               Mobility Gross Bookings of 31% year-over-year.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R70ZN0eKFoo7"
      },
      "outputs": [],
      "source": [
        "## Let's ask the LLM again\n",
        "openai_response = client.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [{'role': 'user', 'content': prompt}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "09ddo9F1Fs-w",
        "outputId": "d0187944-9e12-4794-dfc1-66a9a6070bc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Therefore, Uber's revenue in 2022 was $37.3 billion.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Accessing the generated response from the AI model.\n",
        "openai_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LuE9mCnbR7u"
      },
      "source": [
        "### Interpretation:\n",
        "How is the LLM able to answer the same question now?\n",
        "\n",
        "The LLM can now answer the question accurately because the relevant financial data is explicitly provided in the `retrieved_context`, allowing the model to reference it directly instead of relying on its pre-trained knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfpLPTtDG57l"
      },
      "source": [
        "As you saw in the example above, we\n",
        "\n",
        "- **retrieved** the context from an external source\n",
        "- **augmented** our prompt that passes to the LLM, and\n",
        "- **generated** the response\n",
        "\n",
        "This is Retrieval Augmented Generation in a nutshell!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKzjA8hmHtaA"
      },
      "source": [
        "### Basic RAG app architecture\n",
        "\n",
        "In the previous example, we ***manually*** retrieved the context from the given file which for all purposes is impractical!\n",
        "\n",
        "Therefore, we have to devise a strategy that enables us to:\n",
        "\n",
        "- Take the query from the user\n",
        "- Identify the documents from the external source that might be relevant for the query.\n",
        "- Pass those documents' information as context to the LLM\n",
        "- LLM then generates the final response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msqpnuzuJHYY"
      },
      "source": [
        "To do the above, we can follow a simple standard architecture as shown below (Image source - https://huyenchip.com/2024/07/25/genai-platform.html)\n",
        "\n",
        "<center><img src=\"https://huyenchip.com/assets/pics/genai-platform/3-rag.png\" width=500 height=400/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXm08cxzLsia"
      },
      "source": [
        "In the subsequent demos today, we shall see how to do the same"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}