{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidheesh-p/AI-Learning/blob/master/Market_Research_Agent_RAG_Tavily_Detailed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c67e91",
      "metadata": {
        "id": "74c67e91"
      },
      "source": [
        "# Market Research Agent — RAG (FAISS) + Tavily Web Search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d135869f",
      "metadata": {
        "id": "d135869f"
      },
      "source": [
        "\n",
        "## What you'll learn\n",
        "- Build a FAISS-based retriever from CSV documents (company profiles & trend snippets).  \n",
        "- Integrate a web search tool (Tavily) for fresh, external evidence.  \n",
        "- Design a Planner → Coordinator → Tools → Draft → Reflect loop (single-agent).  \n",
        "- Add JSON memory for traceability and citations.  \n",
        "- Implement a simple evaluation harness and exercises for learners.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82c1be8e",
      "metadata": {
        "id": "82c1be8e"
      },
      "source": [
        "\n",
        "## 1) Setup & Notes about running this notebook\n",
        "- This notebook includes cells that require internet (model downloads, Tavily).  \n",
        "- If you do not have a `TAVILY_API_KEY`, the web search cell will show a safe offline stub.  \n",
        "- The notebook stores two CSVs in `/mnt/data`: `company_profiles_rag_detailed.csv` and `market_trends_rag_detailed.csv`.\n",
        "- We use `sentence-transformers` (`all-MiniLM-L6-v2`) for embeddings and `faiss-cpu` for vector storage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5c5cf15b",
      "metadata": {
        "id": "5c5cf15b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99659592-c57d-4a6b-df69-b2ea040785bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Install these packages if you run the notebook in a fresh environment: faiss-cpu, sentence-transformers, tavily-python, requests, openai, pandas\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# (Run this cell to install packages if needed)\n",
        "# pip install faiss-cpu sentence-transformers tavily-python requests openai pandas\n",
        "print('Install these packages if you run the notebook in a fresh environment: faiss-cpu, sentence-transformers, tavily-python, requests, openai, pandas')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c55ea83",
      "metadata": {
        "id": "2c55ea83"
      },
      "source": [
        "## 2) Load datasets (we included two CSVs for you)\n",
        "\n",
        "Paths:\n",
        "- `/mnt/data/company_profiles_rag_detailed.csv`\n",
        "- `/mnt/data/market_trends_rag_detailed.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "79861dd8",
      "metadata": {
        "id": "79861dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "98347670-0db6-4e77-e825-6c492e16add4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gdrive/My Drive/Market Research Data/company_profiles_rag_detailed.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1554706072.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcompany_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/content/gdrive/My Drive/Market Research Data/company_profiles_rag_detailed.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrend_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/content/gdrive/My Drive/Market Research Data/market_trends_rag_detailed.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcompanies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompany_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrend_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Companies:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/Market Research Data/company_profiles_rag_detailed.csv'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# After mounting, navigate to your file's path in Google Drive\n",
        "# For example, if your file is in \"My Drive/Colab Notebooks/data/my_data.csv\"\n",
        "file_path = '/content/gdrive/My Drive/Colab Notebooks/data/my_data.csv'\n",
        "\n",
        "import pandas as pd\n",
        "company_csv = r\"/content/gdrive/My Drive/Market Research Data/company_profiles_rag_detailed.csv\"\n",
        "trend_csv = r\"/content/gdrive/My Drive/Market Research Data/market_trends_rag_detailed.csv\"\n",
        "companies = pd.read_csv(company_csv)\n",
        "trends = pd.read_csv(trend_csv)\n",
        "print('Companies:')\n",
        "display(companies.head(10))\n",
        "print('\\nTrends:')\n",
        "display(trends.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b8d0fe",
      "metadata": {
        "id": "11b8d0fe"
      },
      "source": [
        "\n",
        "### Quick EDA (Exploratory Data Analysis)\n",
        "We show simple counts and distributions to help learners understand the data shapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89fbdcf6",
      "metadata": {
        "id": "89fbdcf6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Simple EDA\n",
        "print('Companies by Industry:')\n",
        "print(companies['Industry'].value_counts())\n",
        "print('\\nTrends by Industry:')\n",
        "print(trends['Industry'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40949440",
      "metadata": {
        "id": "40949440"
      },
      "source": [
        "\n",
        "## 3) Build the RAG index (Embeddings + FAISS)\n",
        "**Why:** RAG lets the agent retrieve semantically relevant documents from local knowledge (CSV) instead of exact string matching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2283a5d4",
      "metadata": {
        "id": "2283a5d4"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Prepare docs\n",
        "company_docs = companies.apply(lambda r: f\"Company: {r['CompanyName']} | Industry: {r['Industry']} | Desc: {r['Description']}\", axis=1).tolist()\n",
        "trend_docs = trends.apply(lambda r: f\"Industry: {r['Industry']} | Headline: {r['Headline']} | Snippet: {r['Snippet']} | Date: {r['Date']}\", axis=1).tolist()\n",
        "all_docs = company_docs + trend_docs\n",
        "print(f\"Preparing to embed {len(all_docs)} documents\")\n",
        "\n",
        "# Load model and create embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # downloads once\n",
        "embeddings = model.encode(all_docs, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "# Build FAISS index\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(np.array(embeddings).astype('float32'))\n",
        "\n",
        "# id->doc mapping\n",
        "id2doc = {i: d for i, d in enumerate(all_docs)}\n",
        "\n",
        "def rag_search(query, top_k=4):\n",
        "    q_emb = model.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(np.array(q_emb).astype('float32'), top_k)\n",
        "    return [id2doc[int(i)] for i in I[0]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e406979",
      "metadata": {
        "id": "5e406979"
      },
      "source": [
        "### Test RAG retrieval with example queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "223ffc50",
      "metadata": {
        "id": "223ffc50"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('RAG hits for \"healthcare AI\":')\n",
        "for r in rag_search('healthcare AI', top_k=3):\n",
        "    print('-', r[:300])\n",
        "print('\\nRAG hits for \"precision farming\":')\n",
        "for r in rag_search('precision farming', top_k=3):\n",
        "    print('-', r[:300])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a20fd80",
      "metadata": {
        "id": "2a20fd80"
      },
      "source": [
        "\n",
        "## 4) Web Search Tool — Tavily\n",
        "**Why add web search?** RAG is limited to your local snapshots. Adding a web-search tool lets the agent fetch *fresh, time-sensitive* info (news, blog posts, regulatory updates).\n",
        "\n",
        "> Set `TAVILY_API_KEY` as an environment variable. If not set, a stub will be used so the notebook stays runnable offline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7354550",
      "metadata": {
        "id": "c7354550"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, requests, time\n",
        "\n",
        "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
        "def web_search_tavily(query, num_results=3):\n",
        "    if not TAVILY_API_KEY:\n",
        "        # offline stub\n",
        "        return [f\"[stub] No TAVILY_API_KEY set. Would search for: {query}\"]\n",
        "\n",
        "    url = \"https://api.tavily.com/search\"\n",
        "    headers = {\"Content-Type\":\"application/json\"}\n",
        "    payload = {\"api_key\": TAVILY_API_KEY, \"query\": query, \"num_results\": num_results}\n",
        "    resp = requests.post(url, json=payload, headers=headers, timeout=20)\n",
        "    data = resp.json()\n",
        "    results = []\n",
        "    for r in data.get('results', []):\n",
        "        title = r.get('title') or r.get('heading') or ''\n",
        "        snippet = r.get('content') or r.get('snippet') or ''\n",
        "        link = r.get('link') or r.get('url') or ''\n",
        "        results.append({'title': title, 'snippet': snippet, 'link': link})\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4e887f7",
      "metadata": {
        "id": "a4e887f7"
      },
      "source": [
        "### Test Tavily web search (will use stub if API key missing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d582db",
      "metadata": {
        "id": "33d582db"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(web_search_tavily('global AI healthcare market 2025', num_results=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108f3e2f",
      "metadata": {
        "id": "108f3e2f"
      },
      "source": [
        "\n",
        "## 5) Agent Design — Planner → Coordinator → Tools → Draft → Reflect\n",
        "\n",
        "We'll implement a single `MarketResearchAgent` class with:\n",
        "- `planner(query)` → creates an execution plan.\n",
        "- `gather(query)` → decides which tools to call (RAG and/or Tavily) and collects evidence.\n",
        "- `draft(evidence)` → calls an LLM (or Mock) to produce a draft brief.\n",
        "- `critique_and_revise(draft)` → reflection: critique the draft and revise it.\n",
        "- JSON memory for notes, drafts, and citations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ec0ba0",
      "metadata": {
        "id": "42ec0ba0"
      },
      "outputs": [],
      "source": [
        "import os, json, time, re\n",
        "from typing import List, Dict\n",
        "\n",
        "# Minimal JSON memory\n",
        "class JSONMemory:\n",
        "    def __init__(self, path='/mnt/data/agent_memory_rag_tavily.json'):\n",
        "        self.path = path\n",
        "        if not os.path.exists(self.path):\n",
        "            with open(self.path,'w') as f:\n",
        "                json.dump({'notes':[], 'drafts':[], 'citations':[]}, f)\n",
        "    def load(self):\n",
        "        with open(self.path) as f:\n",
        "            return json.load(f)\n",
        "    def write(self, data):\n",
        "        with open(self.path,'w') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "    def add_note(self, note):\n",
        "        d = self.load(); d['notes'].append({'ts':time.time(), 'note':note}); self.write(d)\n",
        "    def add_citation(self, src):\n",
        "        d = self.load(); d['citations'].append(src); self.write(d)\n",
        "    def add_draft(self, draft):\n",
        "        d = self.load(); d['drafts'].append({'ts':time.time(),'draft':draft}); self.write(d)\n",
        "\n",
        "# Mock LLM functions for offline use\n",
        "class MockLLM:\n",
        "    def draft(self, prompt):\n",
        "        # create a short mock draft using heuristics\n",
        "        return \"\"\"Market Brief (mock)\n",
        "- ICP: SMBs and mid-market\n",
        "- Value Prop: Faster time-to-value\n",
        "- Strengths: focused product, low TCO\n",
        "- Risks: competition, pricing pressure\n",
        "- Sources: [rag:0], [web:0]\n",
        "\"\"\"\n",
        "    def critique(self, draft):\n",
        "        return \"CRITIQUE: Add a metric and ensure at least one explicit citation. Clean up fluff.\"\n",
        "    def revise(self, draft, critique):\n",
        "        return draft + \"\\n\\n[Revised: tightened bullets; added metric 20% YoY growth (example)]\"\n",
        "\n",
        "mock_llm = MockLLM()\n",
        "\n",
        "class MarketResearchAgent:\n",
        "    def __init__(self, memory:JSONMemory):\n",
        "        self.memory = memory\n",
        "\n",
        "    def planner(self, query:str)->List[str]:\n",
        "        plan = [\"Run RAG (internal docs)\", \"Run Tavily web search (freshness)\", \"Draft brief\", \"Critique & Revise\"]\n",
        "        self.memory.add_note('planner created')\n",
        "        return plan\n",
        "\n",
        "    def gather(self, query:str, top_k_rag=3, top_k_web=2, prefer_web=False)->List[Dict]:\n",
        "        evidence = []\n",
        "\n",
        "        rag_hits = rag_search(query, top_k=top_k_rag)\n",
        "        for i,h in enumerate(rag_hits):\n",
        "            evidence.append({'type':'rag', 'id':i, 'text':h})\n",
        "            self.memory.add_citation(f'rag:{i}')\n",
        "\n",
        "        web_hits = web_search_tavily(query, num_results=top_k_web)\n",
        "        for i,h in enumerate(web_hits):\n",
        "            evidence.append({\n",
        "                'type':'web',\n",
        "                'id':i,\n",
        "                'text': h.get('snippet') if isinstance(h, dict) else str(h),\n",
        "                'link': h.get('link') if isinstance(h, dict) else None\n",
        "            })\n",
        "            self.memory.add_citation(f'web:{i}')\n",
        "\n",
        "        self.memory.add_note('gathered evidence')\n",
        "        return evidence\n",
        "\n",
        "    def draft(self, query:str, evidence:List[Dict])->str:\n",
        "        # Build prompt\n",
        "        context = '\\n'.join([f\"[{e['type']}:{e['id']}] {e['text']}\" for e in evidence])\n",
        "        prompt = f\"Draft a concise market brief for: {query}\\nUse the evidence below:\\n{context}\\nKeep it 4-8 bullets and include citations.\"\n",
        "        # Call LLM (mock or real)\n",
        "        draft = mock_llm.draft(prompt)\n",
        "        self.memory.add_draft(draft)\n",
        "        return draft\n",
        "\n",
        "    def critique_and_revise(self, draft:str)->str:\n",
        "        critique = mock_llm.critique(draft)\n",
        "        revised = mock_llm.revise(draft, critique)\n",
        "        self.memory.add_draft(revised)\n",
        "        return revised\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a54f35a6",
      "metadata": {
        "id": "a54f35a6"
      },
      "source": [
        "### 6) Example end-to-end run (Planner → Gather → Draft → Revise) — run this cell to see outputs step-by-step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d75ab0",
      "metadata": {
        "id": "90d75ab0"
      },
      "outputs": [],
      "source": [
        "# Make sure memory directory exists before using JSONMemory\n",
        "import os\n",
        "os.makedirs(\"/mnt/data\", exist_ok=True)\n",
        "\n",
        "# Initialize memory + agent\n",
        "mem = JSONMemory(path=\"/mnt/data/agent_memory_rag_tavily.json\")\n",
        "agent = MarketResearchAgent(mem)\n",
        "\n",
        "# Run workflow\n",
        "query = 'Healthcare AI market outlook 2025'\n",
        "print('Plan:')\n",
        "print(agent.planner(query))\n",
        "\n",
        "evidence = agent.gather(query, top_k_rag=3, top_k_web=2)\n",
        "print('\\nCollected Evidence (first 400 chars each):')\n",
        "for e in evidence:\n",
        "    txt = e['text'] if isinstance(e['text'], str) else str(e['text'])\n",
        "    print('-', e['type'], e.get('id'), ':', txt[:400])\n",
        "\n",
        "draft = agent.draft(query, evidence)\n",
        "print('\\nDraft:\\n', draft)\n",
        "\n",
        "final = agent.critique_and_revise(draft)\n",
        "print('\\nFinal Revised Draft:\\n', final)\n",
        "\n",
        "print('\\nMemory snapshot:')\n",
        "print(mem.load())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65f98e04",
      "metadata": {
        "id": "65f98e04"
      },
      "source": [
        "\n",
        "## 7) Evaluation harness\n",
        "We implement a small evaluator that checks:\n",
        "- Relevance (on-topic), Evidence (citations present), Specificity (numbers or years), Clarity (bullets), Freshness (web evidence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295535d8",
      "metadata": {
        "id": "295535d8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re, json\n",
        "def evaluate(text:str, memory_json_path='/mnt/data/agent_memory_rag_tavily.json'):\n",
        "    scores = {}\n",
        "    scores['Relevance'] = 1 if 'health' in text.lower() or 'healthcare' in text.lower() else 0\n",
        "    with open(memory_json_path) as f: mem = json.load(f)\n",
        "    scores['Evidence'] = 1 if len(mem.get('citations',[]))>0 else 0\n",
        "    scores['Specificity'] = 1 if re.search(r'\\d{4}|\\d+%', text) else 0\n",
        "    scores['Clarity'] = 1 if '-' in text else 0\n",
        "    scores['Freshness'] = 1 if any(c.startswith('web:') for c in mem.get('citations',[])) else 0\n",
        "    return scores\n",
        "\n",
        "print('Eval example (from final draft):')\n",
        "# if final exists in the notebook run sequence, you can call evaluate(final)\n",
        "# Here we just show the function\n",
        "print('Call evaluate(final_draft) after running the agent cells to get scores')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28bf5ea7",
      "metadata": {
        "id": "28bf5ea7"
      },
      "source": [
        "\n",
        "## 8) Extensions & exercises\n",
        "- Replace `MockLLM` with OpenAI or another LLM (ensure to secure API keys).  \n",
        "- Add caching for rag_search results and incremental index updates.  \n",
        "- Build a \"Researcher\" agent that runs web searches in parallel and a \"Writer\" agent that drafts.  \n",
        "- Add a Streamlit app to query the agent interactively.  \n",
        "- Add guardrails: schema validation for drafts (pydantic) and rejection rules if no citations.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jtRHmbO_5z0d"
      },
      "id": "jtRHmbO_5z0d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}